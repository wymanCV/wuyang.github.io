<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Wuyang Li </title>

  <meta name="author" content="Dr. Wuyang Li">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon"
    href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Dr. Wuyang Li</name>
                  </p>
                  <p> I will be a postdoctoral fellow at the <a href="https://www.ee.cuhk.edu.hk/~yxyuan/">AIM Group</a>
                    of The
                    Chinese University of Hong Kong (CUHK),
                    supervised by <a href="https://www.ee.cuhk.edu.hk/~yxyuan/">Prof. Yixuan Yuan</a> in 2024. Prior to
                    this, I
                    completed my PhD degree (2020-2023) at the City University of Hong Kong (CityU) <font color="red">
                      with early
                      graduation</font> and my Bachelor's degree (2016-2020) at Tianjin University. I am broadly
                    interested in
                    computer vision, machine learning, and graph-based learning. My PhD research is in line with
                    graph-based learning for object detection. Currently, my primary focus revolves around large
                    vision/language/multi-modal models, including both technical research and practical applications.
                  </p>
                  <p>
                    <font color="red">I am also looking for overseas opportunities in the U.S. in 2024.
                    </font>
                  </p>

                  <!-- üí•üí•üí• -->
                  <p>I have established an startup team to create a fantastic educational product with AIGC. The
                    related
                    technologies consist of language/vision generation. <strong>We have started the project and
                      successfully applied for the seed funding with hundreds of thousands of HK $</strong>. If you are
                    interested in
                    our startup team, please contact me by email üìß!! </li>
                  </p>
                  <p style="text-align:center">
                    <a href="mailto:wuyangli2-c@my.cityu.edu.hk">Email</a> &nbsp/&nbsp
                    <a href="./Resume.pdf">CV</a> &nbsp/&nbsp
                    <!--                <a href="data/JonBarron-bio.txt">Bio</a> &nbsp/&nbsp-->
                    <a href="https://scholar.google.com/citations?user=3Ml_EbAAAAAJ&hl=zh-CN&oi=ao">Google Scholar</a>
                    &nbsp/&nbsp
                    <a href="https://github.com/wymanCV">Github</a>

                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="images/wuyang2.jpg"><img style="width:100%;max-width:100%" alt="profile photo"
                      src="images/wuyang2.jpg" class="hoverZoomLink"></a>

                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>News</heading>
                  <ul>
                    <li>[2023/12] I pass my PhD defense <font color="red"><strong></strong>(early graduation)</strong>!!
                      </font>
                    </li>
                    <li>[2023/08] 1 paper is selected as <font color="red"><strong>ORAL</strong></font> in ICCV.</li>
                    <li>[2023/07] 2 papers are accepted by <strong>ICCV 2023</strong>.</li>
                    <li>[2023/06] <strong>
                        <font color="red">Successfully apply the seed funding for our
                          entrepreneurial team!
                      </strong>
                      </font>
                    </li>
                    <li>[2023/05] 1 paper is accepted by <strong>MICCAI 2023</strong>.</li>
                    <li>[2023/04] 1 paper is accepted by <strong>TNNLS 2023</strong>.</li>
                    <li>[2023/03] 1 paper is accepted by <strong>TMI 2023</strong>.</li>
                    <li>[2023/02] 1 paper is accepted by <strong>CVPR 2023</strong>.</li>
                    <li>[2023/01] 1 paper is accepted by <strong>TPAMI 2023</strong>.</li>
                    <li>[2022/10] 1 paper is accepted by <strong>TMM 2022</strong>.</li>
                    <li>[2022/06] SIGMA appears on <font color="red"><strong>CVPR Best Paper Finalist
                          [33/8161]</strong></font>!
                    </li>
                    <li>[2022/05] 1 paper is accepted by <strong>MICCAI 2022</strong> (<font color="red"><strong>Early
                          Accept,
                          ORAL</strong></font>).</li>
                    <li>[2022/03] 2 papers are accepted by <strong>CVPR 2022</strong> (one <font color="red">
                        <strong>ORAL</strong>
                      </font>).
                    </li>
                    <li>[2021/10] 1 paper is accepted by <strong>AAAI 2022</strong> (<font color="red">
                        <strong>ORAL</strong>
                      </font>).
                    </li>
                    <li>[2021/07] I passed Ph.D. Qualify Examination.</li>
                    <!-- <li>[Jun 2021] One paper accepted by EndoCV 2021 in conjunction with ISBI 2021</li> -->
                    <!-- <li>[Mar 2021] 1<sup>st</sup> place for polyp detection task in Endoscopy Computer Vision Challenge (EndoCV 2021)</li> -->
                  </ul>

                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Conference Papers</heading>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="three">
                    <img src='images/soma.png' width="250">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <!-- <a href=""> -->
                  <papertitle> Novel Scenes & Classes: Towards Adaptive Open-set Object Detection
                  </papertitle>
                  <!-- </a> -->
                  <br>
                  <em>IEEE International Conference on Computer Vision (ICCV)</em>, 2023, <font color="red">
                    <strong>ORAL</strong>
                  </font>
                  <br>
                  <strong>Wuyang Li</strong>, Xiaoqing Guo, Yixuan Yuan
                  <br>
                  <!-- <a>paper</a>/ -->
                  <!-- <a>code</a> -->
                  <a
                    href="https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Novel_Scenes__Classes_Towards_Adaptive_Open-set_Object_Detection_ICCV_2023_paper.pdf">paper</a>
                  /
                  <a href="https://github.com/CityU-AIM-Group/SOMA">codes</a>

                  <p><em>Key Words</em>: Open Set/ Domain Adaptive Object Detection</p>
                  <p><em>Summary</em>: We formulated a real-world friendly setting, called Adaptive Open-set Object
                    Detection (AOOD), which considers both novel scenes and
                    novel classes. Then, we addressed this by selecting subgraphs (motifs) among
                    object queries for a
                    high-order optimization.
                  </p>
                </td>
                <table
                  style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                  <tbody>
                    <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
                      <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="three">
                          <img src='images/MRM.PNG' width="250">
                        </div>
                      </td>
                      <td style="padding:20px;width:75%;vertical-align:middle">
                        <!-- <a href=""> -->
                        <papertitle> MRM: Masked Relation Modeling for Medical Image Pre-Training with Genetics
                        </papertitle>
                        <!-- </a> -->
                        <br>
                        <em>IEEE International Conference on Computer Vision (ICCV)</em>, 2023
                        <br>
                        Qiushi Yang, <strong>Wuyang Li</strong>, Paul Li, Yixuan Yuan
                        <br>
                        <!-- <a>paper</a>/ -->
                        <!-- <a>code</a> -->
                        <a
                          href="https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_MRM_Masked_Relation_Modeling_for_Medical_Image_Pre-Training_with_Genetics_ICCV_2023_paper.pdf">paper</a>
                        /
                        <a href="https://github.com/CityU-AIM-Group/MRM">codes</a>

                        <p><em>Key Words</em>: Multimodal Pretraining</p>
                        <p><em>Summary</em>: We observed the failure in Masked Image Modeling (MIM) when the scale of
                          the
                          informative foreground is limited. To address this, we proposed Masked Relation Modeling
                          (MRM),
                          which masks the cross-token relations for reliable pretraining.
                        </p>
                      </td>

                      <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                          <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
                            <td style="padding:20px;width:25%;vertical-align:middle">
                              <div class="three">
                                <img src='images/anna.png' width="250">
                              </div>
                            </td>
                            <td style="padding:20px;width:75%;vertical-align:middle">
                              <a
                                href="https://openaccess.thecvf.com/content/CVPR2023/html/Li_Adjustment_and_Alignment_for_Unbiased_Open_Set_Domain_Adaptation_CVPR_2023_paper.html">
                                <papertitle> Adjustment and Alignment for Unbiased Open Set Domain Adaptation
                                </papertitle>
                              </a>
                              <br>
                              <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2023
                              <br>
                              <strong>Wuyang Li</strong>, Jie Liu, Bo Han, Yixuan Yuan
                              <br>
                              <!-- <a>paper</a>/ -->
                              <!-- <a>code</a> -->
                              <a
                                href="https://openaccess.thecvf.com/content/CVPR2023/html/Li_Adjustment_and_Alignment_for_Unbiased_Open_Set_Domain_Adaptation_CVPR_2023_paper.html">paper</a>
                              /
                              <a href="https://github.com/CityU-AIM-Group/Anna">codes</a>/
                              <a href="https://www.youtube.com/watch?v=hFZn16ntyXw">Video Presentation</a>
                              <p><em>Key Words</em>: Open Set Domain Adaptation, Causal Theory</p>
                              <p><em>Summary</em>: We decoupled images into base-class and novel-class regions and
                                addreseed
                                the
                                biased learning in the source domain and biased cross-domain transfer with causal
                                theory.
                              </p>
                            </td>
                          </tr>


                          <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
                            <td style="padding:20px;width:25%;vertical-align:middle">
                              <div class="three">
                                <img src='images/sigma.png' width="250">
                              </div>
                            </td>
                            <td style="padding:20px;width:75%;vertical-align:middle">
                              <a href="https://arxiv.org/abs/2203.06398">
                                <papertitle>SIGMA: Semantic-complete Graph Matching for Domain Adaptive Object Detection
                                </papertitle>
                              </a>
                              <br>
                              <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2022, <font
                                color="red">
                                <strong>ORAL, Best Paper Finalist</strong>
                              </font>
                              <br>
                              <strong>Wuyang Li</strong>, Xinyu Liu, Yixuan Yuan
                              <br>
                              <a href="https://arxiv.org/pdf/2203.06398.pdf">paper</a> /
                              <a href="https://github.com/CityU-AIM-Group/SIGMA">codes</a> /
                              <a href="https://zhuanlan.zhihu.com/p/492956292">Áü•‰πé</a>
                              <p><em>Key Words</em>: Domain Adaptive Object Detection, Graph-based Learning</p>
                              <p><em>Summary</em>: We proposed a SemantIc-complete Graph MAtching (SIGMA) framework to
                                address
                                the
                                cross-domain semantic-mismatching with fine-grained domain adaptation.</p>
                            </td>
                          </tr>


                          <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
                            <td style="padding:20px;width:25%;vertical-align:middle">
                              <div class="three">
                                <img src='images/NLTE.png' width="250">
                              </div>
                            </td>
                            <td style="padding:20px;width:75%;vertical-align:middle">

                              <a href="https://arxiv.org/abs/2204.02620">
                                <papertitle>Towards Robust Adaptive Object Detection under Noisy Annotations
                                </papertitle>
                              </a>
                              <br>
                              <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2022
                              <br>
                              Xinyu Liu, <strong>Wuyang Li</strong>, Qiushi Yang, Baopu Li, Yixuan Yuan
                              <br>
                              <a href="https://arxiv.org/abs/2204.02620">paper</a> /
                              <a href="https://github.com/CityU-AIM-Group/NLTE">codes</a>
                              <!--              <p>IEEE Conference on Computer Vision and Pattern Recognition (CVPR 22)</p>-->
                              <p><em>Key Words</em>: Domain adaptive Object Detection, Noisy Label</p>
                              <p><em>Summary</em>: We explored the robust adaptive object detection under noisy
                                annotations
                                and
                                proposed a Noise Latent Transferability Exploration (NLTE) framework to address the
                                issue.</p>
                            </td>
                          </tr>

                          <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
                            <td style="padding:20px;width:25%;vertical-align:middle">
                              <div class="three">
                                <img src='images/scan.png' width="250">
                              </div>
                            </td>
                            <td style="padding:20px;width:75%;vertical-align:middle">
                              <a href="https://www.aaai.org/AAAI22Papers/AAAI-902.LiW.pdf">
                                <papertitle>SCAN: Cross Domain Object Detection with Semantic Conditioned Adaptation
                                </papertitle>
                              </a>
                              <br>
                              <em>The Association for the Advance of Artificial Intelligence (AAAI)</em>, 2022, <font
                                color="red">
                                <strong>ORAL</strong>
                              </font>
                              <br>
                              <strong>Wuyang Li</strong>, Xinyu Liu, Xiwen Yao, Yixuan Yuan
                              <br>
                              <a href="https://www.aaai.org/AAAI22Papers/AAAI-902.LiW.pdf">paper</a> /
                              <a href="https://github.com/CityU-AIM-Group/SCAN">codes</a>
                              <!--              <p>The AAAI Conference on Artificial Intelligence (AAAI 2022) <font color="red"><strong>(ORAL)</strong></font></p>-->
                              <p><em>Key Words</em>: Domain adaptive Object Detection, Graph-based Learning</p>
                              <p><em>Summary</em>: We proposed a Semantic Conditioned AdaptatioN (SCAN) framework to
                                address
                                the
                                sub-optimal categorical alignment for domain adaptive object detection.</p>
                            </td>
                          </tr>


                          <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
                            <td style="padding:20px;width:25%;vertical-align:middle">
                              <div class="three">
                                <img src='images/h2gm2.png' width="250">
                              </div>
                            </td>
                            <td style="padding:20px;width:75%;vertical-align:middle">
                              <a
                                href="https://link.springer.com/content/pdf/10.1007/978-3-031-16452-1_30.pdf?pdf=inline%20link">
                                <papertitle>H2GM: A Hierarchical Hypergraph Matching Framework for Brain Landmark
                                  Alignment
                                </papertitle>
                              </a>
                              <br>
                              <em>Medical Image Computing and Computer Assisted Intervention (MICCAI)</em>, 2023, <font
                                color="red">

                              </font>
                              <br>
                              Zhibin He <strong>Wuyang Li</strong>, Tuo Zhang, Yixuan Yuan
                              <br>
                              <a
                                href="https://link.springer.com/content/pdf/10.1007/978-3-031-16452-1_30.pdf?pdf=inline%20link">paper</a>
                              /
                              <a href="https://github.com/CityU-AIM-Group/FedInI">codes</a>
                              <p><em>Key Words</em>: Gyral Hinges Alignment, Hypergraph Matching</p>
                              <p><em>Summary</em>: We proposed a multi-scale hypergraph matching framework to achieve a
                                robust alignment between the gyral hinges from two brain MRIs.

                              </p>
                            </td>
                          </tr>


                          <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
                            <td style="padding:20px;width:25%;vertical-align:middle">
                              <div class="three">
                                <img src='images/FedInI1.png' width="250">
                              </div>
                            </td>
                            <td style="padding:20px;width:75%;vertical-align:middle">
                              <a
                                href="https://link.springer.com/content/pdf/10.1007/978-3-031-16452-1_30.pdf?pdf=inline%20link">
                                <papertitle>Intervention & Interaction Federated Abnormality Detection with Noisy
                                  Clients
                                </papertitle>
                              </a>
                              <br>
                              <em>Medical Image Computing and Computer Assisted Intervention (MICCAI)</em>, 2022, <font
                                color="red">
                                <strong>ORAL, Early Accept</strong>
                              </font>
                              <br>
                              Xinyu Liu, <strong>Wuyang Li</strong>, Yixuan Yuan
                              <br>
                              <a
                                href="https://link.springer.com/content/pdf/10.1007/978-3-031-16452-1_30.pdf?pdf=inline%20link">paper</a>
                              /
                              <a href="https://github.com/CityU-AIM-Group/FedInI">codes</a>
                              <p><em>Key Words</em>: Federated Learning, Noisy Label, Causal Intervention</p>
                              <p><em>Summary</em>: We proposed to study and explored the recognition bias in federated
                                object detection under
                                noisy annotations
                                , and then addressed the bias with a novel causality-driven framework.
                              </p>
                            </td>
                          </tr>



                          <table
                            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                            <tbody>
                              <tr>
                                <td style="padding:20px;width:100%;vertical-align:middle">
                                  <heading>Journal Papers</heading>
                                </td>
                              </tr>
                            </tbody>
                          </table>

                          <table
                            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                            <tbody>



                              <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                  <div class="three">
                                    <img src='images/sigma_plus_plus.png' width="250">
                                  </div>
                                </td>
                                <td style="padding:20px;width:75%;vertical-align:middle">
                                  <a href="https://ieeexplore.ieee.org/document/10012542">
                                    <papertitle>SIGMA++: Improved Semantic-complete Graph Matching for Domain Adaptive
                                      Object
                                      Detection
                                    </papertitle>
                                  </a>
                                  <br>
                                  <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</em>, 2023,
                                  IF:
                                  24.314
                                  <br>
                                  <strong>Wuyang Li</strong>, Xinyu Liu, Yixuan Yuan
                                  <br>
                                  <a href="https://ieeexplore.ieee.org/document/10012542">paper</a> /
                                  <a href="https://github.com/CityU-AIM-Group/SIGMA/tree/SIGMA++">codes</a>
                                  <!-- <a href="https://zhuanlan.zhihu.com/p/492956292">Áü•‰πé</a> -->
                                  <p><em>Key Words</em>: Domain Adaptive Object Detection, Hypergraph-based Learning</p>
                                  <p><em>Summary</em>: We improved the original graph-matching framework (SIGMA)
                                    with the high-order
                                    hypergraph space, which addresses the cross-domain semantic-misalignment with
                                    fine-grained
                                    domain
                                    adaptation.
                                  </p>
                                </td>

                              <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                  <div class="three">
                                    <img src='images/scan_plus.png' width="250">
                                  </div>
                                </td>
                                <td style="padding:20px;width:75%;vertical-align:middle">
                                  <a href="https://ieeexplore.ieee.org/document/9931144">
                                    <papertitle>SCAN++: Enhanced Semantic Conditioned Adaptation for Domain Adaptive
                                      Object
                                      Detection
                                    </papertitle>
                                  </a>
                                  <br>
                                  <em>IEEE Transactions on Multimedia (TMM)</em>, 2022, IF: 8.182
                                  <br>
                                  <strong>Wuyang Li</strong>, Xinyu Liu, Yixuan Yuan
                                  <br>
                                  <a href="https://ieeexplore.ieee.org/document/9931144">paper</a> /
                                  <a href="https://github.com/CityU-AIM-Group/SCAN/tree/SCAN++">codes</a>
                                  <p><em>Key Words</em>: Domain Adaptive Object Detection, Conditional Kernels
                                  </p>
                                  <p><em>Summary</em>: We proposed an enhanced semantic-condtioned framework to
                                    address the sub-optimal categorical alignment for cross-doamin detection.
                                  </p>
                                </td>
                              </tr>

                              <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                  <div class="three">
                                    <img src='images/htd.jpg' width="250">
                                  </div>
                                </td>
                                <td style="padding:20px;width:75%;vertical-align:middle">
                                  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9615001">
                                    <papertitle>HTD: Heterogeneous Task Decoupling for Two-Stage Object Detection
                                    </papertitle>
                                  </a>
                                  <br>
                                  <em>IEEE Transactions on Image Processing (TIP)</em>, 2022, IF: 11.041
                                  <br>
                                  <strong>Wuyang Li</strong>, Zhen Chen, Baopu Li, Dingwen Zhang, Yixuan Yuan
                                  <br>
                                  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9615001">paper</a> /
                                  <a href="https://github.com/CityU-AIM-Group/HTD">codes</a>
                                  <!--              <p>IEEE Transactions on Image Processing (TIP 2021), IF: 10.856</p>-->
                                  <p><em>Key Words</em>: Generic Object Detection, Graph-based Learning</p>
                                  <p><em>Summary</em>: We proposed a Heterogeneous Task Decoupling (HTD) framework to
                                    disentangle
                                    the
                                    sibling head with Graph Convolutional Network and Convolutional Network of two-stage
                                    detection pipeline .</p>
                                </td>
                              </tr>

                              <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                  <div class="three">
                                    <img src='images/GRAB.png' width="250">
                                  </div>
                                </td>
                                <td style="padding:20px;width:75%;vertical-align:middle">
                                  <a href="https://ieeexplore.ieee.org/document/10132405">
                                    <papertitle>GRAB-Net: Graph-based Boundary-aware Network for Medical Point Cloud
                                      Segmentation
                                    </papertitle>
                                  </a>
                                  <br>
                                  <em>IEEE Transactions on Medical Imaging (TMI)</em>, 2023, IF:
                                  11.037
                                  <br>
                                  Yifan Liu, <strong>Wuyang Li</strong>, Jie Liu, Hui Chen, Yixuan Yuan
                                  <br>
                                  <a href="https://ieeexplore.ieee.org/document/10132405">paper</a> /
                                  <a href="https://github.com/CUHK-AIM-Group/Decoupled-Unbiased-Teacher">codes</a>
                                  <!-- <a href="https://zhuanlan.zhihu.com/p/492956292">Áü•‰πé</a> -->
                                  <p><em>Key Words</em>: Point-cloud Segmentation, Graph-based Learning</p>
                                  <p><em>Summary</em>: We used the graph to model point clouds and addressed the
                                    ambiguous boundary issue via graph-based learning.
                                  </p>
                                </td>



                              <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                  <div class="three">
                                    <img src='images/DUT.png' width="250">
                                  </div>
                                </td>
                                <td style="padding:20px;width:75%;vertical-align:middle">
                                  <a href="https://ieeexplore.ieee.org/document/10132405">
                                    <papertitle>Decoupled Unbiased Teacher for Source-Free Domain Adaptive Medical
                                      Object
                                      Detection
                                    </papertitle>
                                  </a>
                                  <br>
                                  <em>IEEE Transactions on Neural Networks and Learning Systems (TNNLS)</em>, 2023
                                  <br>
                                  Xinyu Liu, <strong>Wuyang Li</strong>, Yixuan Yuan
                                  <br>
                                  <a href="https://ieeexplore.ieee.org/document/10132405">paper</a> /
                                  <a href="https://github.com/CUHK-AIM-Group/Decoupled-Unbiased-Teacher">codes</a>
                                  <!-- <a href="https://zhuanlan.zhihu.com/p/492956292">Áü•‰πé</a> -->
                                  <p><em>Key Words</em>: Source-free Domain Adaptive Object Detection, Causal Theory</p>
                                  <p><em>Summary</em>: We leveraged causal theory to explore the bias problem in SFDA,
                                    and
                                    addressed
                                    the sample bias, feature bias, and prediction bias.
                                  </p>
                                </td>




                                <table
                                  style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                                  <tbody>
                                    <tr>
                                      <td style="padding:20px;width:100%;vertical-align:middle">
                                        <heading>Reviewer</heading>
                                        <ul>
                                          <li>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)
                                          </li>
                                          <li>International Journal of Computer Visio (IJCV) </li>
                                          <li>IEEE Transactions on Neural Networks and Learning Systems (TNNLS) </li>
                                          <li>IEEE Transactions on Image Processing (TIP) </li>
                                          <li>IEEE Transactions on Automation Science and Engineering (TASE) </li>
                                          <li>Pattern Recognition (PR) </li>
                                          <li>IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023,
                                            2024 </li>
                                          <li>IEEE International Conference on Computer Vision (ICCV), 2023 </li>
                                          <li>AAAI Conference on Artificial Intelligence (AAAI), 2023, 2024 </li>
                                        </ul>
                                      </td>
                                    </tr>
                                  </tbody>
                                </table>


                                <table
                                  style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                                  <tbody>
                                    <tr>
                                      <td style="padding:20px;width:100%;vertical-align:middle">
                                        <heading>Selected Honors</heading>
                                        <ul>
                                          <li>[2023] Outstanding Academic Performance Award (OAPA), CityU </li>
                                          <li>[2023] Research Tuition Scholarship (RTS), CityU </li>
                                          <li>[2022] Outstanding Academic Performance Award (OAPA), CityU </li>
                                          <li>[2022] Research Tuition Scholarship (RTS), CityU </li>
                                          <li> [2018] National Scholarship (Top 2% student) </li>
                                          <li> [2017] National Scholarship (Top 2% student) </li>
                                          <li> [2017] Tianjin Mathematical Competition (Second Prize) </li>
                                          <li> [2017-2020] Outstanding Student Scholarship (Top 10% student) </li>
                                        </ul>
                                      </td>
                                    </tr>
                                  </tbody>
                                </table>




                                <table
                                  style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                                  <tbody>
                                    <tr>
                                      <td style="padding:20px;width:100%;vertical-align:middle">
                                        <heading>Education</heading>
                                      </td>
                                    </tr>
                                  </tbody>
                                </table>
                                <table
                                  style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                                  <tbody>
                                    <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">

                                      <td style="padding:40px;width:1%;vertical-align:middle">
                                        <div class="three">

                                          <img src='images/tju.png' width="100">
                                        </div>
                                      </td>
                                      <td style="padding:20px;width:75%;vertical-align:middle">
                                        <a href="http://www.tju.edu.cn/english/index.htm">
                                          <papertitle>Tianjin University (TJU), China</papertitle>
                                        </a>
                                        <br>
                                        <p>Sep. 2016 - Jun. 2020Ôºö Bachelor's degree of Communication Engineering. </p>
                                        <p>GPA: 3.83/4.00, 91.3/100, Ranking 6/120</p>
                                      </td>
                                    </tr>

                                    <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">

                                      <td style="padding:50px;width:1%;vertical-align:middle">
                                        <div class="three">
                                          <img src='images/nusri.png' width="80">
                                        </div>
                                      </td>
                                      <td style="padding:20px;width:75%;vertical-align:middle">
                                        <a href="http://en.nusri.cn/">
                                          <papertitle>NUS (Suzhou) Research Institute (NUSRI), China</papertitle>
                                        </a>
                                        <br>
                                        <p>Sep. 2019 - Jun. 2020: Exchanging program of Electrical and Computer
                                          Engineering.
                                        </p>
                                        <p>Supervisors: Prof. Zhiying Zhou</p>
                                        <p>Complete the project: Towards Webpage-based Augmentation Reality (AR)</p>
                                      </td>
                                    </tr>

                                    <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">

                                      <td style="padding:0px;width:1%;vertical-align:middle">
                                        <div class="three">
                                          <img src='images/cityu.png' width="180">
                                        </div>
                                      </td>
                                      <td style="padding:20px;width:75%;vertical-align:middle">
                                        <a href="https://www.cityu.edu.hk/">
                                          <papertitle>City University of Hong Kong (CityU), China</papertitle>
                                        </a>
                                        <br>
                                        <p>Sep. 2020 - present: Ph.D Study of Electrical Engineering. </p>
                                        <p>Supervisors: Prof. Yixuan Yuan</p>
                                        <!--              <p>Complete the project: Towards Webpage-based Augmentation Reality (AR)</p>-->
                                      </td>
                                    </tr>




                                    <table
                                      style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                                      <tbody>
                                        <tr>
                                          <td style="padding:20px;width:100%;vertical-align:middle">
                                            <heading>Leadership Experience</heading>
                                          </td>
                                        </tr>
                                      </tbody>
                                    </table>
                                    <table
                                      style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                                      <tbody>
                                        <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">

                                          <td style="padding:10px;width:1%;vertical-align:middle">
                                            <div class="three">
                                              <img src='images/leader1.png' width="200">
                                            </div>
                                          </td>
                                          <td style="padding:20px;width:75%;vertical-align:middle">
                                            <a>
                                              <papertitle>Freshman Leader, Tianjin University</papertitle>
                                            </a>
                                            <br>
                                            <p>Jun. 2017 - Jun. 2018: Freshman leader for Class 2, communication
                                              engineering
                                            </p>
                                            <p>I was fortunate to be one of eight freshman leaders selected through the
                                              departmet.</p>
                                          </td>
                                        </tr>

                                        <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">

                                          <td style="padding:60px;width:1%;vertical-align:middle">
                                            <div class="three">
                                              <img src='images/leader2.jpg' width="100">
                                            </div>
                                          </td>
                                          <td style="padding:20px;width:75%;vertical-align:middle">
                                            <a>
                                              <papertitle> Student Union Chairman of Electrical and Information
                                                Engineering
                                                Department,
                                                Tianjin University</papertitle>
                                            </a>
                                            <br>
                                            <p>Sep. 2018 - Jun. 2020: Chairman of the publicity department. </p>

                                            <p> I was fortunate to be selected as the publicity department chairman of
                                              the
                                              student
                                              union
                                              in Electrical and Information Engineering Department, Tianjin University.
                                            </p>
                                            <!--               <p>Supervisors: Prof. Zhiying Zhou</p>-->
                                            <!--              <p>Complete the project: Towards Webpage-based Augmentation Reality (AR)</p>-->
                                          </td>
                                        </tr>






                                        <table
                                          style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                                          <tbody>
                                            <tr>
                                              <td style="padding:20px;width:100%;vertical-align:middle">
                                                <heading>Personal Interests</heading>
                                                <ul>
                                                  <li> Painting and Desinging: I used to do sketch training with art
                                                    candidates
                                                    and
                                                    have a
                                                    certain level of graphic design foundation. I have a strong interest
                                                    in user
                                                    needs
                                                    analysis and product design. </li>
                                                  <li> I am looking for the opportunity to establish a start-up team and
                                                    create
                                                    some
                                                    awesome high-tech products.</li>
                                                </ul>
                                              </td>
                                            </tr>
                                          </tbody>
                                        </table>






                                        <table
                                          style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                                          <tbody>

                                            <table
                                              style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                                              <tbody>
                                                <tr>
                                                  <td style="padding:0px">
                                                    <br>
                                                    <a style="text-align:right;font-size:small;">
                                                      <a href="https://jonbarron.info/">We steal this website from this
                                                        guy</a>
                                                      </p>
                                                  </td>
                                                </tr>
                                              </tbody>
                                            </table>
        </td>
      </tr>
  </table>
</body>

</html>